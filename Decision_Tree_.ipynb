{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans\n",
        "\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning algorithm used for classification and regression tasks. In classification, it is used to predict a category or class label based on input features. It works like a flowchart structure where each internal node represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents the final class label.\n",
        "\n",
        "In classification, the algorithm starts at the root node and splits the data based on the feature that best separates the classes. It uses measures like **Gini Index** or **Entropy (Information Gain)** to decide the best feature for splitting. The process continues recursively, creating smaller branches until the data is clearly classified or a stopping condition is reached.\n",
        "\n",
        "For example, if we are predicting whether a loan should be approved or not, the tree might first split based on income level, then credit score, and then employment status. By following the decisions from root to leaf, the model assigns a final class label.\n",
        "\n",
        "In simple terms, a Decision Tree classifies data by asking a series of questions and following the answers step by step until it reaches a final decision.\n"
      ],
      "metadata": {
        "id": "-4SZSwnIsuGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans\n",
        "\n",
        "**Gini Impurity** and **Entropy** are impurity measures used in Decision Trees to decide the best feature for splitting the data. They measure how mixed the classes are in a node. A pure node contains data points from only one class, while an impure node contains a mix of classes.\n",
        "\n",
        "**Gini Impurity** measures the probability of incorrectly classifying a randomly chosen data point if it were randomly labeled according to the class distribution in the node. Its value ranges from 0 to 0. A Gini value of 0 means the node is completely pure (all samples belong to one class). The Decision Tree algorithm selects the split that results in the lowest Gini impurity.\n",
        "\n",
        "**Entropy** measures the amount of uncertainty or randomness in the data. It ranges from 0 (completely pure) to 1 (maximum disorder, for binary classification). Entropy is used to calculate **Information Gain**, which shows how much uncertainty is reduced after a split. The split with the highest Information Gain (or lowest Entropy) is chosen.\n",
        "\n",
        "Both Gini Impurity and Entropy help the Decision Tree choose the best feature to split the data. They impact the splits by ensuring that each split makes the resulting nodes as pure as possible, leading to better classification performance.\n"
      ],
      "metadata": {
        "id": "HdNmOPE5s8So"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Pre-Pruning** and **Post-Pruning** are techniques used to prevent overfitting in Decision Trees.\n",
        "\n",
        "**Pre-Pruning (Early Stopping)** stops the tree from growing before it becomes too complex. It sets conditions such as maximum depth, minimum number of samples required to split a node, or minimum impurity decrease. If these conditions are met, the tree stops splitting further.\n",
        "**Practical Advantage:** It reduces training time and computational cost because the tree is controlled during the building process.\n",
        "\n",
        "**Post-Pruning (Backward Pruning)** allows the tree to grow fully and then removes branches that do not significantly improve model performance. It trims unnecessary nodes after evaluating them using validation data or cost-complexity pruning.\n",
        "**Practical Advantage:** It often produces a more accurate and better-generalized model because pruning decisions are made after seeing the full tree structure.\n",
        "\n",
        "In short, pre-pruning controls growth early to save resources, while post-pruning simplifies a fully grown tree to improve generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7cO-1IitFWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Information Gain** is a measure used in Decision Trees to decide which feature should be chosen for splitting the data. It is based on the concept of **Entropy**, which measures the uncertainty or impurity in a dataset.\n",
        "\n",
        "Information Gain calculates how much the entropy decreases after splitting the data on a particular feature. In simple terms, it measures how much “information” a feature provides about the class label.\n",
        "\n",
        "If a split results in child nodes that are more pure (less mixed), the entropy decreases significantly, and the Information Gain is high.\n",
        "\n",
        "It is important because the Decision Tree algorithm selects the feature with the **highest Information Gain** for splitting. This ensures that each split makes the data more organized and closer to pure class labels, leading to better and more accurate classification.\n"
      ],
      "metadata": {
        "id": "B81e9NdGtXRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans\n",
        "\n",
        "Decision Trees are used in many real-life applications because they are simple and easy to understand.\n",
        "\n",
        "Common applications include loan approval, where banks decide whether to give a loan based on income and credit score; medical diagnosis, where symptoms are used to predict diseases; fraud detection, where suspicious transactions are identified; customer churn prediction, where companies find customers who may leave; and marketing, where businesses group customers for targeted advertising.\n",
        "\n",
        "The main advantages of Decision Trees are that they are easy to understand and explain, can handle both numerical and categorical data, require less data preparation, and are useful for decision-making problems.\n",
        "\n",
        "The main limitations are that they can easily overfit if the tree becomes too large, small changes in data can create a very different tree, and they may not be as accurate as advanced models like Random Forest or Gradient Boosting.\n",
        "\n",
        "In simple terms, Decision Trees are useful and easy-to-use models, but they must be carefully controlled to avoid overfitting and instability.\n"
      ],
      "metadata": {
        "id": "jAJ6zvsSto6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "lUMqldUuwF5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset (built-in dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree model using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fU3m9lYwJZ2",
        "outputId": "daa6cafa-1795-4113-df73-802de979cc89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "tK2MGkRBxHoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset (built-in dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fully-grown Decision Tree\n",
        "# -----------------------------\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# -----------------------------\n",
        "# Decision Tree with max_depth=3\n",
        "# -----------------------------\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Fully-Grown Tree:\", accuracy_full)\n",
        "print(\"Accuracy of Tree with max_depth=3:\", accuracy_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xleDETPDx77u",
        "outputId": "335390fc-3fcc-4789-d40a-9f8bffc955cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-Grown Tree: 1.0\n",
            "Accuracy of Tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importance"
      ],
      "metadata": {
        "id": "C0Mk8rEtz8jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load Boston Housing dataset from OpenML\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "\n",
        "X = boston.data\n",
        "y = boston.target.astype(float)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRnOX3A1z-G1",
        "outputId": "21ca5a92-b5c1-47cb-f008-16aaa416a0dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.588026315789474\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.05846545229060361\n",
            "ZN: 0.000988919249451643\n",
            "INDUS: 0.009872448809169472\n",
            "CHAS: 0.0002973342835618114\n",
            "NOX: 0.007050562083191356\n",
            "RM: 0.575807411273885\n",
            "AGE: 0.007170198655228184\n",
            "DIS: 0.10962404854314393\n",
            "RAD: 0.001646356693641641\n",
            "TAX: 0.002181112508453187\n",
            "PTRATIO: 0.025042865841170155\n",
            "B: 0.011872990423277916\n",
            "LSTAT: 0.189980299345222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "AawT65JS0E5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train using GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict using best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ysUOp90FiN",
        "outputId": "fd9f0c19-3fa4-401a-c9b0-e855fdca55ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 6}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans\n",
        "\n",
        "If I were working as a data scientist in a healthcare company, I would follow a structured step-by-step approach:\n",
        "\n",
        "Step 1: Handle Missing Values\n",
        "First, I would analyze how many missing values exist and in which columns.\n",
        "\n",
        "* For numerical features (like age, blood pressure), I would fill missing values using mean or median.\n",
        "* For categorical features (like gender, disease history), I would use mode (most frequent value).\n",
        "  If missing values are very high in a column, I might consider dropping that feature.\n",
        "  In advanced cases, I could use techniques like KNN imputation.\n",
        "\n",
        "Step 2: Encode Categorical Features\n",
        "Machine learning models cannot understand text categories directly.\n",
        "\n",
        "* For binary categories (Yes/No), I would use Label Encoding.\n",
        "* For features with multiple categories (e.g., blood type), I would use One-Hot Encoding.\n",
        "  This converts categorical data into numerical format suitable for the Decision Tree.\n",
        "\n",
        "Step 3: Split the Dataset\n",
        "I would split the dataset into training and testing sets (for example, 80% training and 20% testing) to evaluate performance properly.\n",
        "\n",
        "Step 4: Train the Decision Tree Model\n",
        "I would create a DecisionTreeClassifier and train it using the cleaned and encoded training data. Decision Trees are useful because they handle mixed data types well and are easy to interpret.\n",
        "\n",
        "Step 5: Tune Hyperparameters\n",
        "To avoid overfitting, I would tune parameters such as:\n",
        "\n",
        "* max_depth\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* criterion (gini or entropy)\n",
        "\n",
        "I would use GridSearchCV or RandomizedSearchCV to find the best combination of parameters.\n",
        "\n",
        "Step 6: Evaluate the Model\n",
        "I would evaluate performance using:\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* Confusion Matrix\n",
        "* ROC-AUC score\n",
        "\n",
        "In healthcare, Recall is very important because missing a diseased patient (false negative) can be dangerous.\n",
        "\n",
        "Step 7: Interpret the Model\n",
        "Decision Trees allow us to see feature importance. This helps doctors understand which factors (e.g., blood pressure, cholesterol, age) contribute most to the disease prediction.\n",
        "\n",
        "Business Value in Real-World Healthcare\n",
        "\n",
        "This model can provide several benefits:\n",
        "\n",
        "* Early disease detection, leading to timely treatment\n",
        "* Reduced healthcare costs by identifying high-risk patients early\n",
        "* Better resource allocation in hospitals\n",
        "* Data-driven decision support for doctors\n",
        "* Improved patient outcomes\n",
        "\n",
        "In simple terms, this model can help the healthcare company detect diseases faster, improve treatment planning, and potentially save lives while reducing operational costs.\n"
      ],
      "metadata": {
        "id": "DlgZmrwS0RuJ"
      }
    }
  ]
}