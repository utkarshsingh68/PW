{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners\n",
        "\n",
        "Ans\n",
        "\n",
        "**Boosting** is an ensemble learning technique in machine learning where multiple weak learners are combined to form a strong learner. A weak learner is a model that performs slightly better than random guessing, such as a shallow decision tree (decision stump).\n",
        "\n",
        "Boosting works by training models sequentially. In each step, the algorithm focuses more on the data points that were misclassified by the previous model. It assigns higher importance (weights) to those difficult examples so that the next model tries harder to correct them. After training several weak learners, their predictions are combined (usually using weighted voting or weighted averaging) to produce the final result.\n",
        "\n",
        "By repeatedly correcting mistakes and giving more attention to hard cases, boosting reduces bias and improves overall accuracy. As a result, many simple models together can perform much better than a single weak model.\n"
      ],
      "metadata": {
        "id": "uUc5U_H74beE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**AdaBoost** and **Gradient Boosting** are both boosting algorithms, but they differ in how they train models and correct errors.\n",
        "\n",
        "In **AdaBoost**, models are trained sequentially, and after each model is trained, the algorithm increases the weights of the misclassified data points. This means the next model focuses more on the mistakes made by the previous one. Each weak learner is given a weight based on its accuracy, and the final prediction is made using weighted voting (for classification) or weighted averaging (for regression).\n",
        "\n",
        "In **Gradient Boosting**, models are also trained sequentially, but instead of changing data weights directly, each new model is trained to predict the residual errors (the difference between actual and predicted values) of the previous model. It uses gradient descent to minimize a loss function step by step.\n",
        "\n",
        "In simple terms, AdaBoost focuses on adjusting the weights of misclassified data points, while Gradient Boosting focuses on minimizing a loss function by learning from previous errors.\n"
      ],
      "metadata": {
        "id": "hITUnjco4fXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Regularization in XGBoost** helps prevent overfitting and makes the model more generalizable to new data.\n",
        "\n",
        "XGBoost includes regularization directly in its objective function. It adds a penalty term that controls the complexity of the model. This penalty discourages the model from creating overly complex trees that fit the training data too closely.\n",
        "\n",
        "There are two main types of regularization in XGBoost:\n",
        "\n",
        "* **L1 regularization (alpha)**: Encourages sparsity by reducing some feature weights to zero. This helps in feature selection.\n",
        "* **L2 regularization (lambda)**: Penalizes large weights and keeps the model more stable.\n",
        "\n",
        "In addition, XGBoost also controls complexity using parameters like:\n",
        "\n",
        "* `max_depth` (limits tree depth)\n",
        "* `min_child_weight`\n",
        "* `subsample` (uses random subset of data)\n",
        "* `colsample_bytree` (uses subset of features)\n",
        "\n",
        "By controlling model complexity, regularization reduces variance, prevents overfitting, and improves performance on unseen data. In simple terms, regularization helps XGBoost build a strong but not overly complex model.\n"
      ],
      "metadata": {
        "id": "gmYCtbR_4pGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Ans\n",
        "\n",
        "**CatBoost** is considered efficient for handling categorical data because it can process categorical features directly without requiring manual encoding like One-Hot Encoding or Label Encoding.\n",
        "\n",
        "Traditional machine learning models require categorical data to be converted into numerical form before training. This can increase dimensionality and sometimes reduce model performance. However, CatBoost uses a special technique called **ordered target encoding**, which converts categorical variables into numerical values in a smart and unbiased way.\n",
        "\n",
        "CatBoost also:\n",
        "\n",
        "* Handles high-cardinality categorical features efficiently.\n",
        "* Reduces overfitting caused by improper encoding.\n",
        "* Automatically detects which features are categorical.\n",
        "* Uses an ordered boosting technique to prevent data leakage.\n",
        "\n",
        "Because of these built-in mechanisms, CatBoost simplifies preprocessing, improves performance, and is especially powerful for datasets with many categorical features, such as financial or customer data.\n"
      ],
      "metadata": {
        "id": "e93iY1f643yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "Ans\n",
        "\n",
        "Boosting techniques are preferred over bagging methods in real-world applications where **high prediction accuracy** is more important than just reducing variance, and where the data has complex patterns.\n",
        "\n",
        "Some common real-world applications include:\n",
        "\n",
        "1. Loan Default Prediction\n",
        "   In banking and finance, boosting methods like XGBoost or LightGBM are often used to predict whether a customer will default on a loan. These problems are complex and require models that can capture subtle patterns in customer behavior.\n",
        "\n",
        "2. Fraud Detection\n",
        "   Boosting works well in fraud detection because it focuses on correcting previous mistakes and pays attention to difficult or rare cases, which is important in detecting fraudulent transactions.\n",
        "\n",
        "3. Medical Diagnosis\n",
        "   In healthcare, boosting is used for disease prediction or risk assessment because it provides high accuracy and handles complex relationships between features.\n",
        "\n",
        "4. Recommendation Systems\n",
        "   Boosting can improve prediction accuracy for ranking and recommendation tasks by learning from errors step by step.\n",
        "\n",
        "5. Competition-Level Machine Learning\n",
        "   In platforms like Kaggle competitions, boosting algorithms (especially XGBoost, LightGBM, and CatBoost) are often preferred because they consistently deliver high performance.\n",
        "\n",
        "In simple terms, boosting is preferred over bagging when the goal is to achieve the best possible accuracy on complex problems, especially in high-stakes domains like finance, healthcare, and fraud detection.\n"
      ],
      "metadata": {
        "id": "fhqIDQIE5VyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accurac"
      ],
      "metadata": {
        "id": "2P2qzq3r5euU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create AdaBoost Classifier\n",
        "model = AdaBoostClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BTYyLqKe5hwE",
        "outputId": "395da1e4-1c41-4aef-a1de-0d499c4efe3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared scor"
      ],
      "metadata": {
        "id": "iIB4zM295sVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(\"R-squared Score:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AVDbo5bB5tHh",
        "outputId": "232bf5cb-c593-4614-b1be-29120d41c800"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7803012822391022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "p57jABZd5u0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xgboost if not installed\n",
        "# pip install xgboost\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create XGBoost classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid (tuning learning rate)\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train model using grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XgrbNV545zWt",
        "outputId": "1aeb35e5-b6f0-4d4c-a6fd-67c91a4885e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:12] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:14] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:19] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:23] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:27] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:32] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:32] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [21:17:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.3}\n",
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n"
      ],
      "metadata": {
        "id": "m8oI07wF57yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install catboost if not installed\n",
        "!pip install catboost\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix using seaborn\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "-MtV_1Y258WR",
        "outputId": "f4cb08be-d424-4595-f277-4a00ff239e5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.10-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas<4.0,>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<4.0,>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<4.0,>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<4.0,>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.4)\n",
            "Downloading catboost-1.2.10-cp312-cp312-manylinux2014_x86_64.whl (97.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.10\n",
            "Model Accuracy: 0.9707602339181286\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "If I am working in a FinTech company to predict loan default, I would follow a structured pipeline using boosting techniques to handle the imbalanced and mixed-type dataset.\n",
        "\n",
        "Step 1: Data Preprocessing\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        "* For numerical features (income, transaction amount), I would use median imputation.\n",
        "* For categorical features (employment type, marital status), I would use mode imputation or a separate “Missing” category.\n",
        "* If missingness is meaningful, I may create an indicator feature.\n",
        "\n",
        "2. Handle Categorical Variables\n",
        "\n",
        "* If using XGBoost or AdaBoost, I would apply One-Hot Encoding or Target Encoding.\n",
        "* If using CatBoost, I would directly pass categorical features, since CatBoost handles them internally and efficiently.\n",
        "\n",
        "3. Handle Imbalanced Data\n",
        "   Loan default datasets are usually imbalanced (few defaulters).\n",
        "   I would:\n",
        "\n",
        "* Use class weights (scale_pos_weight in XGBoost).\n",
        "* Try SMOTE or oversampling if needed.\n",
        "* Focus on evaluation metrics beyond accuracy.\n",
        "\n",
        "4. Feature Engineering\n",
        "\n",
        "* Create transaction behavior features (average monthly spend, repayment ratio, credit utilization rate).\n",
        "* Normalize or scale features if required (though tree-based models usually do not require scaling).\n",
        "\n",
        "Step 2: Choice Between AdaBoost, XGBoost, and CatBoost\n",
        "\n",
        "* AdaBoost: Simple and useful for smaller datasets, but less powerful for complex financial data.\n",
        "* XGBoost: Highly powerful, handles missing values internally, supports regularization, and performs well in structured tabular data.\n",
        "* CatBoost: Best when there are many categorical variables, as it handles them automatically and reduces data leakage.\n",
        "\n",
        "For loan default prediction with many categorical financial features, I would prefer CatBoost or XGBoost.\n",
        "\n",
        "Step 3: Hyperparameter Tuning Strategy\n",
        "\n",
        "I would use:\n",
        "\n",
        "* GridSearchCV or RandomizedSearchCV for structured tuning.\n",
        "* Cross-validation (k-fold) to ensure stability.\n",
        "\n",
        "Important hyperparameters to tune:\n",
        "\n",
        "* learning_rate\n",
        "* n_estimators\n",
        "* max_depth\n",
        "* subsample\n",
        "* colsample_bytree\n",
        "* regularization parameters\n",
        "\n",
        "I may also use early stopping to prevent overfitting.\n",
        "\n",
        "Step 4: Evaluation Metrics\n",
        "\n",
        "Since the dataset is imbalanced, accuracy alone is not sufficient.\n",
        "\n",
        "I would focus on:\n",
        "\n",
        "* Precision: To reduce false positives (approving risky customers).\n",
        "* Recall: To reduce false negatives (missing actual defaulters).\n",
        "* F1-score: Balance between precision and recall.\n",
        "* ROC-AUC: Measures overall classification performance.\n",
        "* Confusion Matrix: To understand type of errors.\n",
        "\n",
        "In finance, recall for defaulters is very important because missing a defaulter can cause financial loss.\n",
        "\n",
        "Step 5: Business Benefits\n",
        "\n",
        "This model would provide:\n",
        "\n",
        "* Better risk assessment before loan approval.\n",
        "* Reduced financial losses from loan defaults.\n",
        "* Improved credit decision automation.\n",
        "* Data-driven customer risk scoring.\n",
        "* Competitive advantage through smarter lending.\n",
        "\n",
        "In real-world finance, boosting models improve prediction accuracy, reduce risk exposure, and help the company make safer and more profitable lending decisions.\n"
      ],
      "metadata": {
        "id": "wPNSVa6p5_Ml"
      }
    }
  ]
}