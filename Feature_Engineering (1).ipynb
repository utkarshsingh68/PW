{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 What is a parameter?\n",
        "Ans\n",
        "A parameter is a numerical value that describes some characteristic of a population.\n",
        "\n",
        "Example: If we want to study the average height of all students in your university, the actual average height of the whole university (everyone included) is called a parameter.\n",
        "\n",
        "It’s usually fixed but unknown, because we can’t always measure the whole population. That’s why we often take a sample and calculate a statistic (like sample mean), which is used to estimate the parameter.\n",
        "\n",
        "So,\n",
        "\n",
        "Parameter → Population characteristic (fixed, usually unknown).\n",
        "\n",
        "Statistic → Sample characteristic (calculated, used to estimate parameter)."
      ],
      "metadata": {
        "id": "lggkJzwR2ALk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 What is correlation?\n",
        " What does negative correlation mean?\n",
        "Ans\n",
        "Correlation is a statistical measure that shows the strength and direction of the relationship between two variables.\n",
        "\n",
        "It tells us whether the variables increase together, decrease together, or move in opposite directions.\n",
        "\n",
        "The correlation coefficient (usually denoted by r) ranges between –1 and +1.\n",
        "\n",
        "\n",
        "Negative correlation means that when one variable increases, the other variable decreases.\n",
        "\n",
        "The value of r will be less than 0 (e.g., –0.7).\n",
        "\n",
        "Example: The number of hours spent watching TV and exam scores. Generally, if TV time goes up, exam scores go down.\n"
      ],
      "metadata": {
        "id": "ySNzZqOh2XYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans\n",
        "The loss value tells us how far off our model’s predictions are from the actual (true) values.\n",
        "\n",
        "A low loss value means the predictions are close to the real answers → the model is performing well.\n",
        "\n",
        "A high loss value means the predictions are very different from the real answers → the model is performing poorly.\n",
        "\n",
        "In training machine learning models, we check the loss after each step (or epoch). If the loss keeps decreasing, it shows the model is learning properly. If the loss stays high or doesn’t improve, the model might not be good, or it might be underfitting/overfitting."
      ],
      "metadata": {
        "id": "Wa1v6tJk3Epw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 What are continuous and categorical variables ?\n",
        "Ans\n",
        "Continuous Variables\n",
        "\n",
        "A continuous variable can take any numerical value within a range.\n",
        "\n",
        "They are measurable and can have fractions/decimals.\n",
        "\n",
        "Example: Height (170.5 cm), weight (62.3 kg), temperature (36.7°C).\n",
        "\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "A categorical variable represents groups or categories instead of exact numbers.\n",
        "\n",
        "They are not measured, but classified/labelled.\n",
        "\n",
        "Example: Gender (male/female/other), blood group (A, B, AB, O), colors (red, blue, green).\n",
        "\n"
      ],
      "metadata": {
        "id": "KrCu9PyG3XqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "Ans\n",
        "Machine learning models usually work with numbers, not text labels. So, we need to convert categorical variables into numerical form before using them in models.\n",
        "\n",
        "Common Techniques\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Assigns each category a unique number.\n",
        "\n",
        "Example: Gender → Male = 0, Female = 1.\n",
        "\n",
        "Problem: It may wrongly introduce “order” (model thinks 0 < 1 < 2).\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Creates a new binary column for each category (1 = present, 0 = not present).\n",
        "\n",
        "Example: Color → [Red, Blue, Green] →\n",
        "\n",
        "Red = [1,0,0], Blue = [0,1,0], Green = [0,0,1].\n",
        "\n",
        "Works well, but increases dataset size if categories are many.\n",
        "\n",
        "Ordinal Encoding\n",
        "\n",
        "Used when categories have a natural order.\n",
        "\n",
        "Example: Education → High School = 1, Graduate = 2, Postgraduate = 3.\n",
        "\n",
        "Target Encoding (Mean Encoding) (advanced)\n",
        "\n",
        "Replace a category with the average of the target variable for that category.\n",
        "\n",
        "Example: If in “City A” average sales = 200, replace City A with 200."
      ],
      "metadata": {
        "id": "ZKVu2BmD3tG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7  What do you mean by training and testing a dataset?\n",
        "Ans\n",
        "\n",
        "In machine learning, we split our dataset into two parts:\n",
        "\n",
        "Training Dataset\n",
        "\n",
        "This part of the data is used to teach the model.\n",
        "\n",
        "The model looks at the input features and the correct output (labels) to learn patterns.\n",
        "\n",
        "Example: If we’re predicting house prices, the training data has house details (size, location, rooms) and their actual prices.\n",
        "\n",
        "Testing Dataset\n",
        "\n",
        "This part of the data is kept aside and not shown to the model during training.\n",
        "\n",
        "After the model is trained, we use the testing data to check how well the model performs on unseen data.\n",
        "\n",
        "Example: Give the model new house details and see if its predicted price is close to the real price.\n"
      ],
      "metadata": {
        "id": "1_2FYgAD3--j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 What is sklearn.preprocessing?\n",
        "Ans\n",
        "sklearn.preprocessing is a module in scikit-learn (sklearn) that provides tools to prepare and transform raw data before feeding it into a machine learning model.\n",
        "\n",
        "Since most models work better when data is in a certain format (like normalized, scaled, or encoded), preprocessing makes the data clean, consistent, and machine-friendly.\n",
        "\n",
        "Common Functions in sklearn.preprocessing\n",
        "\n",
        "Scaling & Normalization\n",
        "\n",
        "StandardScaler → makes data have mean = 0 and variance = 1.\n",
        "\n",
        "MinMaxScaler → scales values between 0 and 1.\n",
        "\n",
        "Normalizer → scales row-wise (useful in text classification).\n",
        "\n",
        "Encoding Categorical Data\n",
        "\n",
        "LabelEncoder → converts categories into numbers.\n",
        "\n",
        "OneHotEncoder → converts categories into binary vectors.\n",
        "\n",
        "Feature Transformation\n",
        "\n",
        "PolynomialFeatures → generates polynomial combinations (useful in polynomial regression).\n",
        "\n",
        "Binarizer → turns values into 0/1 based on a threshold."
      ],
      "metadata": {
        "id": "TEJCAUoU4J6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 What is a Test set?\n",
        "Ans\n",
        "A test set is the portion of a dataset that is kept aside and not used while training the model.\n",
        "\n",
        "After the model has been trained on the training data, we use the test set to check how well the model can make predictions on unseen data.\n",
        "\n",
        "It helps us measure the model’s accuracy, performance, and generalization ability.\n",
        "\n",
        "Suppose we have 1,000 student exam records.\n",
        "\n",
        "We use 800 records for training (teaching the model the pattern between study hours and marks).\n",
        "\n",
        "The remaining 200 records form the test set (to check if the model can predict marks correctly for new students)."
      ],
      "metadata": {
        "id": "52R2BlMq4o_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "Ans\n",
        "In Python, we commonly use scikit-learn’s train_test_split function to divide data into training and testing sets."
      ],
      "metadata": {
        "id": "pzaGo0Qb497i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "P-1MXrn85LvP",
        "outputId": "057379d5-f372-4e78-b238-9f99fb337cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1294826043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02c6c407",
        "outputId": "49f33523-2a96-42a5-d956-cf7c7017fcfa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4, 2)\n",
            "X_test shape: (1, 2)\n",
            "y_train shape: (4,)\n",
            "y_test shape: (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_size=0.2 → 20% for testing, 80% for training.\n",
        "\n",
        "random_state → ensures reproducibility (same split each time).\n",
        "\n",
        "Q2: How do you approach a Machine Learning problem ?\n",
        "\n",
        "The general approach to solving an ML problem can be broken into steps:\n",
        "\n",
        "Understand the Problem\n",
        "\n",
        "Define the objective (e.g., predict house prices, classify diseases).\n",
        "\n",
        "Identify type of ML task: classification, regression, clustering, etc.\n",
        "\n",
        "Collect & Explore Data\n",
        "\n",
        "Gather dataset.\n",
        "\n",
        "Use EDA (Exploratory Data Analysis) → check for missing values, outliers, distributions.\n",
        "\n",
        "Preprocess the Data\n",
        "\n",
        "Handle missing values.\n",
        "\n",
        "Encode categorical variables.\n",
        "\n",
        "Scale/normalize numerical features.\n",
        "\n",
        "Split Data\n",
        "\n",
        "Use training set (to build model) and testing set (to evaluate model).\n",
        "\n",
        "Choose and Train a Model\n",
        "\n",
        "Try suitable algorithms (Linear Regression, Decision Trees, Random Forest, Neural Networks).\n",
        "\n",
        "Evaluate Model\n",
        "\n",
        "Use metrics (accuracy, precision, recall, RMSE, etc.).\n",
        "\n",
        "Compare different models.\n",
        "\n",
        "Tune the Model\n",
        "\n",
        "Use techniques like hyperparameter tuning (GridSearchCV, RandomSearchCV).\n",
        "\n",
        "Deploy the Model\n",
        "\n",
        "Integrate into real-world applications (e.g., web app, mobile app)."
      ],
      "metadata": {
        "id": "uyBtet1M5iui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11 Why do we have to perform EDA before fitting a model to the data?\n",
        "Ans\n",
        "EDA (Exploratory Data Analysis) is the process of exploring and understanding a dataset before building a machine learning model.\n",
        "\n",
        "We perform EDA because:\n",
        "\n",
        "Understand Data Structure\n",
        "\n",
        "Know the number of rows, columns, and types of variables (categorical, continuous, etc.).\n",
        "\n",
        "Detect Missing Values & Outliers\n",
        "\n",
        "Helps identify incomplete or unusual data that can affect model performance.\n",
        "\n",
        "Check Data Distribution\n",
        "\n",
        "Some models assume normal distribution; EDA helps confirm this.\n",
        "\n",
        "Find Relationships Between Variables\n",
        "\n",
        "Correlations and patterns can guide feature selection.\n",
        "\n",
        "Avoid Garbage In, Garbage Out\n",
        "\n",
        "If the data is messy, the model will give poor results, no matter how advanced it is.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose you are predicting house prices:\n",
        "\n",
        "EDA might show that “location” strongly affects price.\n",
        "\n",
        "It might also reveal missing values in “number of rooms,” which you need to fix before training."
      ],
      "metadata": {
        "id": "aKhzD-f86Jav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 What is correlation?\n",
        "Ans\n",
        "Correlation is a statistical measure that shows the strength and direction of the relationship between two variables.\n",
        "\n",
        "It tells us how closely two variables move together.\n",
        "\n",
        "The correlation value (denoted as r) lies between –1 and +1.\n",
        "\n",
        "Types of Correlation\n",
        "\n",
        "Positive Correlation (+r):\n",
        "\n",
        "When one variable increases, the other also increases.\n",
        "\n",
        "Example: Hours studied ⬆ → Marks scored ⬆.\n",
        "\n",
        "Negative Correlation (–r):\n",
        "\n",
        "When one variable increases, the other decreases.\n",
        "\n",
        "Example: Watching TV hours ⬆ → Exam scores ⬇.\n",
        "\n",
        "Zero Correlation (r ≈ 0):\n",
        "\n",
        "No relationship between variables.\n",
        "\n",
        "Example: Shoe size and intelligence."
      ],
      "metadata": {
        "id": "FRfKuE1x6b4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13 What does Negative Correlation mean?\n",
        "Ans\n",
        "Negative correlation means that when one variable increases, the other variable decreases.\n",
        "\n",
        "The correlation coefficient (r) will be less than 0 (ranges from –1 to 0).\n",
        "\n",
        "The closer r is to –1, the stronger the negative relationship.\n",
        " Example:\n",
        "\n",
        "TV watching hours vs. Exam scores → More TV  → Lower marks .\n",
        "\n",
        "Speed of a car vs. Travel time → Higher speed  → Less travel time ."
      ],
      "metadata": {
        "id": "-hsI3LgI6o8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14 How can you find correlation between variables in Python?\n",
        "\n",
        "Ans\n",
        "\n",
        "In Python, we mainly use the Pandas and Seaborn/Matplotlib libraries to calculate and visualize correlation."
      ],
      "metadata": {
        "id": "z5XrvbzE7KeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Hours_Studied': [2, 3, 4, 5, 6],\n",
        "    'Marks': [50, 60, 65, 80, 85]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.corr())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyz6myQR7WBU",
        "outputId": "3a43c5c2-619b-4366-de04-91160c20259f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Hours_Studied     Marks\n",
            "Hours_Studied       1.000000  0.987878\n",
            "Marks               0.987878  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "XTJ80agh7d65",
        "outputId": "aa1e7ae8-d69c-43e9-c384-2105924f2d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "corr, p_value = pearsonr(df['Hours_Studied'], df['Marks'])\n",
        "print(\"Correlation:\", corr)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJMNXm817jXJ",
        "outputId": "d2121fff-74f2-4e94-cfd5-1cbe6df6b957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation: 0.9878783399072133\n",
            "p-value: 0.0015991373885635045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Ans\n",
        "\n",
        "What is Causation?\n",
        "\n",
        "Causation means that one variable produces a direct effect on another variable.\n",
        "\n",
        "In simple terms → A causes B.\n",
        "\n",
        "It’s a cause-and-effect relationship, not just an association.\n",
        "\n",
        "Example: If you push a ball (cause), it rolls (effect).\n",
        "\n",
        "What is Correlation?\n",
        "\n",
        "Correlation means that two variables are related (they move together in some way), but one may not cause the other.\n",
        "\n",
        "Example: As height increases, weight also tends to increase. (They are related but height does not directly cause weight).\n",
        "\n",
        "Correlation but NOT causation:\n",
        "\n",
        "In summer, ice cream sales increase  and drowning accidents increase .\n",
        "\n",
        "They are correlated because both rise in hot weather, but eating ice cream doesn’t cause drowning.\n",
        "\n",
        "Causation (real cause-effect):\n",
        "\n",
        "Smoking cigarettes → damages lungs → leads to lung cancer.\n",
        "\n",
        "Here, smoking causes cancer (not just a random relationship)."
      ],
      "metadata": {
        "id": "ZEoy59eU7oMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans\n",
        "\n",
        "In Machine Learning and Deep Learning, an optimizer is an algorithm that updates the weights of a model during training in order to minimize the loss function.\n",
        "\n",
        "The model makes predictions.\n",
        "\n",
        "Loss function calculates the error.\n",
        "\n",
        "Optimizer adjusts the model weights step by step to reduce this error.\n",
        "\n",
        "1. Gradient Descent\n",
        "\n",
        "The most basic optimizer.\n",
        "\n",
        "It calculates the gradient (slope) of the loss function and updates weights in the opposite direction of the gradient.\n",
        "\n",
        "\n",
        "Example:\n",
        "Predicting house prices → weights (coefficients) are updated after each step to minimize prediction error.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Instead of using the entire dataset, it updates weights for each data point (or small batch).\n",
        "\n",
        "Faster than normal Gradient Descent, but more noisy.\n",
        "\n",
        "Example:\n",
        "Training an image classifier → updates weights after looking at 1 (or few) images at a time.\n",
        "\n",
        "3. Momentum\n",
        "\n",
        "Improves SGD by adding a memory of past updates.\n",
        "\n",
        "Helps to move faster in the correct direction and avoid oscillations.\n",
        "\n",
        "Example:\n",
        "Like rolling a ball down a hill—it gains momentum and doesn’t get stuck in small bumps.\n",
        "\n",
        "4. Adagrad (Adaptive Gradient)\n",
        "\n",
        "Adjusts the learning rate for each parameter individually.\n",
        "\n",
        "Works well for sparse data (many zeros).\n",
        "\n",
        "Example:\n",
        "NLP (natural language processing) tasks where many words are rare → Adagrad adjusts learning for each word differently.\n",
        "\n",
        "5. RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "Similar to Adagrad but solves its problem of decreasing learning rate too much.\n",
        "\n",
        "Keeps learning rate adaptive and efficient.\n",
        "\n",
        "Example:\n",
        "Widely used in Recurrent Neural Networks (RNNs) for sequence data like text or time series.\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Combines Momentum + RMSprop.\n",
        "\n",
        "The most popular optimizer in deep learning because it’s fast and usually works well without much tuning.\n",
        "\n",
        "Example:\n",
        "Used in almost all deep learning tasks → image recognition, NLP, speech recognition, etc."
      ],
      "metadata": {
        "id": "Jl6O52SX8L5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 What is sklearn.linear_model ?\n",
        "\n",
        "Ans\n",
        "\n",
        "sklearn.linear_model is a module in the Python library scikit-learn (sklearn) that provides classes and functions for implementing linear models for machine learning tasks like regression and classification.\n",
        "\n",
        "Linear models assume that the relationship between the input features (X) and the target variable (y) can be represented as a linear combination of the input features.\n",
        "\n",
        "Key Algorithms inside sklearn.linear_model:\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "Class: LinearRegression\n",
        "\n",
        "Used for predicting continuous values.\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Class: LogisticRegression\n",
        "\n",
        "Used for binary or multi-class classification problems.\n",
        "\n",
        "Outputs probabilities using the logistic (sigmoid) function.\n",
        "\n",
        "Ridge Regression (L2 regularization)\n",
        "\n",
        "Class: Ridge\n",
        "\n",
        "Adds a penalty term proportional to the square of the coefficients to prevent overfitting.\n",
        "\n",
        "Lasso Regression (L1 regularization)\n",
        "\n",
        "Class: Lasso\n",
        "\n",
        "Shrinks coefficients and can set some to zero, performing feature selection.\n",
        "\n",
        "Elastic Net\n",
        "\n",
        "Class: ElasticNet\n",
        "\n",
        "Combines both L1 (Lasso) and L2 (Ridge) penalties.\n",
        "\n",
        "SGD Classifiers and Regressors\n",
        "\n",
        "Classes: SGDClassifier, SGDRegressor\n",
        "\n",
        "Use Stochastic Gradient Descent to fit linear models efficiently for large datasets.\n",
        "\n",
        "Perceptron\n",
        "\n",
        "Class: Perceptron\n",
        "\n",
        "A simple linear classifier used in early neural networks.\n"
      ],
      "metadata": {
        "id": "3Cb_o9UW8fj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.predict([[6]]))  # Output: [6.]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uYLGILd8z1n",
        "outputId": "4c0ffcca-8e76-4868-a3ae-89251f3b49f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18 What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans\n",
        "\n",
        " What model.fit() does:\n",
        "\n",
        "Takes in training data (X) and target labels/values (y).\n",
        "\n",
        "Learns the relationship between them (i.e., finds model parameters like weights and bias).\n",
        "\n",
        "Stores these learned parameters inside the model object so you can later call .predict() or .score().\n",
        "\n",
        " Arguments of .fit()\n",
        "\n",
        "Required arguments (almost always):\n",
        "\n",
        "X:\n",
        "\n",
        "Input features (independent variables).\n",
        "\n",
        "Must be array-like, shape = (n_samples, n_features).\n",
        "\n",
        "Example: For 100 students with 2 features (hours studied, hours slept) → shape (100, 2).\n",
        "\n",
        "y:\n",
        "\n",
        "Target values (dependent variable).\n",
        "\n",
        "For regression → continuous values.\n",
        "\n",
        "For classification → class labels (0/1, or multiclass).\n",
        "\n",
        "Shape = (n_samples,) or (n_samples, n_outputs).\n",
        "\n",
        "Optional arguments (depend on the model):\n",
        "\n",
        "sample_weight:\n",
        "\n",
        "Some models (like LinearRegression, LogisticRegression) accept it.\n",
        "\n",
        "Lets you assign different importance (weights) to different samples.\n",
        "\n",
        "Shape = (n_samples,).\n",
        "\n",
        "Example: If some data points are more reliable, you give them higher weights."
      ],
      "metadata": {
        "id": "k7zg-4VM89Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([40, 50, 60, 70, 80])\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5rV0nUp9IJr",
        "outputId": "8987db19-a885-46bb-aa98-f42f07c80d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient: [10.]\n",
            "Intercept: 30.000000000000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19 What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans\n",
        "\n",
        " What model.predict() does:\n",
        "\n",
        "Takes input data (X_new) with the same structure/features as the training data.\n",
        "\n",
        "Applies the learned model parameters (from .fit()) to compute predictions.\n",
        "\n",
        "Returns the predicted output (y_pred).\n",
        "\n",
        "For regression → continuous values (e.g., house prices).\n",
        "\n",
        "For classification → predicted class labels (e.g., 0 or 1).\n",
        "\n",
        "Arguments of .predict()\n",
        "\n",
        "Required:\n",
        "\n",
        "X:\n",
        "\n",
        "The data you want predictions for.\n",
        "\n",
        "Must be array-like, shape = (n_samples, n_features).\n",
        "\n",
        "Even if predicting for a single sample, it should be 2D (e.g., [[6]], not just 6).\n",
        "\n",
        "No y needed here → because .predict() is only for inputs, it doesn’t require true labels."
      ],
      "metadata": {
        "id": "g-agc5fA9QXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([40, 50, 60, 70, 80])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "prediction = model.predict([[6]])\n",
        "print(\"Predicted marks:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqHmN4Ia9hJw",
        "outputId": "45fe0da3-ba37-49b1-ff5b-0ce744638fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted marks: [90.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 5], [2, 4], [3, 6], [4, 3], [5, 7]])\n",
        "y = np.array([0, 0, 1, 1, 1])  # 0 = Fail, 1 = Pass\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.predict([[3, 5]]))  # Output: [1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSfEPK1z9l7B",
        "outputId": "3c4f01b0-3d7c-4a32-8b2a-a22e5531b25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20 What are continuous and categorical variables?\n",
        "\n",
        "Ans\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "Variables that can take any numeric value within a range.\n",
        "\n",
        "They are measured, not counted.\n",
        "\n",
        "Can be fractions/decimals (not just whole numbers).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 170.5 cm)\n",
        "\n",
        "Weight (e.g., 62.8 kg)\n",
        "\n",
        "Temperature (e.g., 36.7 °C)\n",
        "\n",
        "Time taken to run (e.g., 12.34 seconds)\n",
        "\n",
        "They are usually used in regression problems.\n",
        "\n",
        " Categorical Variables\n",
        "\n",
        "Variables that represent categories or groups.\n",
        "\n",
        "They are qualitative, not quantitative.\n",
        "\n",
        "Can be:\n",
        "\n",
        "Nominal (no order) – categories without ranking.\n",
        "\n",
        "Example: Colors (Red, Blue, Green), Gender (Male, Female, Other), City names.\n",
        "\n",
        "Ordinal (ordered categories) – categories with a meaningful order/ranking.\n",
        "\n",
        "Example: Education level (Primary < Secondary < College < PhD), Ratings (Poor < Average < Good < Excellent)."
      ],
      "metadata": {
        "id": "XO90TjyD9rA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21 🔹 What is Feature Scaling?\n",
        "\n",
        "Feature scaling is the process of transforming the values of numerical features into a similar range so that no single feature dominates others just because of its scale (magnitude of values).\n",
        "\n",
        "For example:\n",
        "\n",
        "Height = 170 cm\n",
        "\n",
        "Salary = ₹50,000\n",
        "\n",
        "Here, \"salary\" has much larger numbers than \"height\". Some algorithms might treat salary as more important just because the numbers are bigger, even if it isn’t truly more important.\n",
        "\n",
        "Why is Feature Scaling Important?\n",
        "\n",
        "Improves model performance\n",
        "\n",
        "Algorithms like KNN, K-means, SVM, Neural Networks are distance-based.\n",
        "\n",
        "If one feature has larger values, it will dominate distance calculations.\n",
        "\n",
        "Scaling ensures fair contribution from all features.\n",
        "\n",
        "Speeds up training\n",
        "\n",
        "In Gradient Descent–based algorithms (like Linear/Logistic Regression, Neural Networks), scaling helps the model converge faster.\n",
        "\n",
        "Without scaling, optimization might take much longer.\n",
        "\n",
        "Ensures coefficients are comparable\n",
        "\n",
        "In linear models (like Linear Regression, Logistic Regression, Ridge, Lasso), scaling makes feature coefficients easier to interpret.\n",
        "\n",
        "Common Methods of Feature Scaling\n",
        "\n",
        "Min-Max Normalization (Rescaling)\n",
        "\n",
        "Scales values to [0, 1] range.\n",
        "\n",
        "Example: Heights between 150–200 cm → scaled to 0–1.\n",
        "\n",
        "Standardization (Z-score scaling)\n",
        "\n",
        "\n",
        "Transforms values to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "Useful when data has positive & negative values.\n",
        "\n",
        "Robust Scaling\n",
        "\n",
        "Uses median and IQR (Interquartile Range).\n",
        "\n",
        "More robust to outliers."
      ],
      "metadata": {
        "id": "0PO3FqKH-WuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 How do we perform scaling in Python?\n",
        "\n",
        "Ans\n",
        "\n",
        "Methods of Scaling in Python\n",
        "1. Min-Max Scaling (Normalization)\n",
        "\n",
        "Scales features to a fixed range, usually [0, 1]"
      ],
      "metadata": {
        "id": "pP_70a5t-o9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[170, 50000],\n",
        "              [180, 60000],\n",
        "              [160, 40000]])\n",
        "\n",
        "scaler = MinMaxScaler()   # default range = [0, 1]\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Normalized Data:\\n\", X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IiOQDId-3I6",
        "outputId": "6f6315db-15f2-487d-cc54-63c77c332a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data:\n",
            " [[0.5 0.5]\n",
            " [1.  1. ]\n",
            " [0.  0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardization (Z-score Scaling)\n",
        "\n",
        "Transforms features to have mean = 0 and std = 1."
      ],
      "metadata": {
        "id": "eNQEW7zQ-65x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Standardized Data:\\n\", X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFdr2q7F-8Dq",
        "outputId": "c5133696-d8e8-4e00-b4a1-861e668c6a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]\n",
            " [-1.22474487 -1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 What is sklearn.preprocessing?\n",
        "\n",
        "Ans\n",
        "\n",
        "sklearn.preprocessing is a module in scikit-learn that provides tools to prepare and transform raw data before feeding it into machine learning models.\n",
        "\n",
        "Since most ML algorithms assume data is clean, numerical, and scaled, sklearn.preprocessing helps by:\n",
        "\n",
        "Scaling / Normalizing features\n",
        "\n",
        "Encoding categorical variables\n",
        "\n",
        "Generating polynomial features\n",
        "\n",
        "Handling missing values (imputation tools are in sklearn.impute)\n",
        "\n",
        "Basically, it contains data preprocessing utilities to make raw data model-friendly.\n",
        "\n",
        " Key Functions & Classes in sklearn.preprocessing\n",
        "1. Scaling & Normalization\n",
        "\n",
        "StandardScaler → standardizes data (mean=0, std=1).\n",
        "\n",
        "MinMaxScaler → scales values to [0,1].\n",
        "\n",
        "RobustScaler → scales using median & IQR (good for outliers).\n",
        "\n",
        "MaxAbsScaler → scales to [-1, 1] by max absolute value.\n",
        "\n",
        "Normalizer → scales each row to have unit norm (for text/cosine similarity).\n",
        "\n",
        "2. Encoding Categorical Data\n",
        "\n",
        "LabelEncoder → encodes labels (e.g., “red, green, blue” → 0,1,2).\n",
        "\n",
        "OneHotEncoder → creates binary dummy variables (e.g., “red, green, blue” → [1,0,0], [0,1,0], [0,0,1]).\n",
        "\n",
        "OrdinalEncoder → encodes categories with an order (e.g., small < medium < large → 0,1,2).\n",
        "\n",
        "3. Feature Engineering\n",
        "\n",
        "PolynomialFeatures → generates polynomial combinations\\\n",
        "\n",
        "Binarizer → converts values above a threshold into 1, else 0.\n",
        "\n",
        "4. Other Utilities\n",
        "\n",
        "FunctionTransformer → lets you apply custom transformations.\n",
        "\n",
        "PowerTransformer → makes data more Gaussian-like (Box-Cox, Yeo-Johnson transforms).\n",
        "\n",
        "QuantileTransformer → maps data to a uniform or normal distribution."
      ],
      "metadata": {
        "id": "q6UbMXkS_JxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 200],\n",
        "              [2, 300],\n",
        "              [3, 400]])\n",
        "scaler = StandardScaler()\n",
        "print(\"Scaled:\\n\", scaler.fit_transform(X))\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "colors = np.array([[\"Red\"], [\"Green\"], [\"Blue\"], [\"Red\"]])\n",
        "print(\"One-hot:\\n\", encoder.fit_transform(colors).toarray())\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "print(\"Polynomial:\\n\", poly.fit_transform([[2, 3]]))  # expands [2,3] → [1,2,3,4,6,9]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzpNQf0M_T_H",
        "outputId": "71edb58e-0dcc-42cc-a5a4-4c50603ffaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n",
            "One-hot:\n",
            " [[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n",
            "Polynomial:\n",
            " [[1. 2. 3. 4. 6. 9.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans\n",
        "\n",
        "Why Split Data?\n",
        "\n",
        "Training set → used to fit (train) the model.\n",
        "\n",
        "Testing set → used to evaluate the model on unseen data.\n",
        "\n",
        "This prevents overfitting (when a model memorizes training data but fails on new data).\n",
        " How to Split in Python (scikit-learn)\n",
        "\n",
        "We use train_test_split from sklearn.model_selection."
      ],
      "metadata": {
        "id": "yPoa7AFO_hpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60, 70, 80])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train:\\n\", X_train)\n",
        "print(\"X_test:\\n\", X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ldOfQaa_tY-",
        "outputId": "e99a08e1-4ec0-4924-aa4e-474d3cf93075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:\n",
            " [[8]\n",
            " [3]\n",
            " [5]\n",
            " [4]\n",
            " [7]]\n",
            "X_test:\n",
            " [[2]\n",
            " [6]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters of train_test_split\n",
        "\n",
        "X, y → Features and target values.\n",
        "\n",
        "test_size → Fraction or number of samples for testing (e.g., 0.2 = 20%).\n",
        "\n",
        "train_size → Fraction for training (optional, since it’s 1 - test_size).\n",
        "\n",
        "random_state → Random seed (for reproducibility).\n",
        "\n",
        "shuffle=True → Shuffles data before splitting (default is True).\n",
        "\n",
        "stratify=y → Ensures the same proportion of classes in train & test (useful in classification)."
      ],
      "metadata": {
        "id": "C6ABMOJA_3HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1], [2], [3], [4], [5], [6]]\n",
        "y = [0, 0, 1, 1, 1, 0]  # Class labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train labels:\", y_train)\n",
        "print(\"Test labels:\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69myIfvE_5Ex",
        "outputId": "180675d3-7aff-4f6f-fb88-f06f38170e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels: [0, 1, 0, 1]\n",
            "Test labels: [1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 Explain data encoding?\n",
        "\n",
        "Ans\n",
        "\n",
        "Data encoding is the process of converting categorical (non-numeric) variables into a numerical format so that machine learning algorithms can understand and process them.\n",
        "\n",
        " Most ML models (like Linear Regression, Logistic Regression, SVM, Neural Networks) only work with numbers, not text labels.\n",
        "\n",
        "Example:\n",
        "\n",
        "Raw data: [\"Red\", \"Blue\", \"Green\"]\n",
        "\n",
        "Encoded data: [0, 1, 2] or one-hot vectors like [[1,0,0], [0,1,0], [0,0,1]].\n",
        "\n",
        "Types of Data Encoding\n",
        "1. Label Encoding\n",
        "\n",
        "Converts categories into numeric labels (0, 1, 2, …).\n",
        "\n",
        "Example: [\"Red\", \"Blue\", \"Green\"] → [2, 0, 1].\n",
        "\n",
        "Problem: Algorithms might think Green (1) < Red (2), which may not make sense for nominal data."
      ],
      "metadata": {
        "id": "yhbiOV66ABYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "colors = [\"Red\", \"Blue\", \"Green\", \"Red\"]\n",
        "encoder = LabelEncoder()\n",
        "encoded = encoder.fit_transform(colors)\n",
        "print(encoded)  # [2 0 1 2]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrgnJmPOALvV",
        "outputId": "b765fd34-8d6d-4b3a-9201-d1e57b17f2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One-Hot Encoding\n",
        "\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Colors = [\"Red\", \"Blue\", \"Green\"]\n",
        "\n",
        "Encoded ="
      ],
      "metadata": {
        "id": "0RJBCUBsAPra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "colors = np.array([[\"Red\"], [\"Blue\"], [\"Green\"], [\"Red\"]])\n",
        "encoder = OneHotEncoder()\n",
        "encoded = encoder.fit_transform(colors).toarray()\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWsUovviAQpP",
        "outputId": "31ed8f68-921c-4603-ae27-57b287b50f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e84748"
      },
      "source": [
        "2. One-Hot Encoding\n",
        "\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Colors = [\"Red\", \"Blue\", \"Green\"]\n",
        "\n",
        "Encoded =\n",
        "\n",
        "| Red | Blue | Green |\n",
        "|---|---|---|\n",
        "| 1 | 0 | 0 |\n",
        "| 0 | 1 | 0 |\n",
        "| 0 | 0 | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "colors = np.array([[\"Red\"], [\"Blue\"], [\"Green\"], [\"Red\"]])\n",
        "encoder = OneHotEncoder()\n",
        "encoded = encoder.fit_transform(colors).toarray()\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RocbhqHAY2h",
        "outputId": "d163c9e0-b6bc-4f14-9ab5-e2f8b0fabf8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ordinal Encoding\n",
        "\n",
        "Assigns ordered numbers when categories have a natural order.\n",
        "\n",
        "Example: Education = [\"High School\", \"Bachelor\", \"Master\", \"PhD\"] → [0,1,2,3].\n",
        "\n",
        "Useful when order matters."
      ],
      "metadata": {
        "id": "7QDGX5w2Ab0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "levels = [[\"High School\"], [\"Bachelor\"], [\"Master\"], [\"PhD\"]]\n",
        "encoder = OrdinalEncoder()\n",
        "encoded = encoder.fit_transform(levels)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5gxSNClAedq",
        "outputId": "54a1f7ad-40d8-4e9e-a816-a9a1d0725ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [0.]\n",
            " [2.]\n",
            " [3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Binary / Hash Encoding (advanced, for high-cardinality data)\n",
        "\n",
        "Converts categories into binary numbers or hashes.\n",
        "\n",
        "Useful when there are thousands of unique categories (like zip codes, product IDs).\n",
        "\n",
        "(Libraries like category_encoders in Python provide these.)\n",
        "\n",
        "Choosing Encoding\n",
        "\n",
        "Nominal categories (no order): One-Hot Encoding.\n",
        "\n",
        "Ordinal categories (with order): Ordinal Encoding.\n",
        "\n",
        "High-cardinality features: Hashing / Target encoding.\n",
        "\n",
        "Simple models (trees): Often don’t need encoding (they can handle labels directly)."
      ],
      "metadata": {
        "id": "jRFtmHX-AjUV"
      }
    }
  ]
}