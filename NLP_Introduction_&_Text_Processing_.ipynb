{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Ans\n",
        "\n",
        "**Computational Linguistics** is an interdisciplinary field that studies how computers can understand, analyze, and generate human language using linguistic rules and computational methods. It combines knowledge from linguistics (such as grammar, syntax, and semantics) and computer science to model language in a structured way.\n",
        "\n",
        "Computational Linguistics is closely related to **Natural Language Processing (NLP)**. While Computational Linguistics focuses more on the theoretical and rule-based understanding of language, NLP focuses on practical applications that use machine learning and algorithms to process language data. In simple terms, Computational Linguistics provides the linguistic foundation, and NLP applies those concepts to build real-world applications like chatbots, machine translation, speech recognition, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "PL7g5-Rihi9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "Ans\n",
        "\n",
        "The historical evolution of Natural Language Processing (NLP) can be divided into different stages. In the 1950s and 1960s, NLP started with **rule-based systems**, where researchers used hand-written grammar rules for tasks like machine translation. These systems were limited because they required extensive manual effort and could not handle language variations well.\n",
        "\n",
        "In the 1980s and 1990s, NLP shifted toward **statistical methods**, where machine learning techniques and probability models were used to analyze large text datasets. Algorithms like Hidden Markov Models (HMM) and n-grams became popular during this time.\n",
        "\n",
        "In the 2000s, **machine learning-based approaches** became more advanced with the availability of large datasets and better computing power. Techniques such as Support Vector Machines (SVM) and decision trees were widely used.\n",
        "\n",
        "After 2010, NLP entered the **deep learning era**, where neural networks like RNNs, LSTMs, and later Transformers significantly improved performance in tasks like translation, text generation, and sentiment analysis. Today, large pre-trained language models like BERT and GPT have made NLP more powerful and accurate than ever before.\n"
      ],
      "metadata": {
        "id": "oXa2lFclhqMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "Three major use cases of NLP in today’s tech industry are **Chatbots and Virtual Assistants, Sentiment Analysis, and Machine Translation**.\n",
        "\n",
        "**1. Chatbots and Virtual Assistants:**\n",
        "NLP is used to build chatbots and voice assistants that can understand and respond to human language. These systems process user queries and provide relevant answers. For example, customer support chatbots on websites and virtual assistants like Alexa or Google Assistant use NLP to interact with users naturally.\n",
        "\n",
        "**2. Sentiment Analysis:**\n",
        "Sentiment analysis uses NLP to determine whether a piece of text expresses a positive, negative, or neutral opinion. Companies use it to analyze customer reviews, social media posts, and feedback to understand customer satisfaction and improve products or services.\n",
        "\n",
        "**3. Machine Translation:**\n",
        "NLP enables automatic translation of text from one language to another. Applications like Google Translate use NLP techniques to translate sentences accurately while maintaining meaning and context.\n",
        "\n",
        "These use cases show how NLP is widely applied in communication, business, and global technology platforms.\n"
      ],
      "metadata": {
        "id": "-NQ8BT2ch2-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Ans\n",
        "\n",
        "**Text normalization** is the process of converting raw text into a clean and consistent format so that it can be easily processed by machine learning models. It involves steps such as converting text to lowercase, removing punctuation and special characters, correcting spelling, removing stop words, and sometimes applying stemming or lemmatization.\n",
        "\n",
        "Text normalization is essential in text processing tasks because raw text often contains variations that can confuse models. For example, words like “Running,” “running,” and “RUNNING” should be treated as the same word. By normalizing text, we reduce noise, improve data quality, and make the model more accurate and efficient. It helps in better feature extraction and improves the overall performance of NLP applications such as sentiment analysis, chatbots, and text classification.\n"
      ],
      "metadata": {
        "id": "UFazXjXQiBTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "\n",
        "Ans\n",
        "\n",
        "**Stemming** and **Lemmatization** are text preprocessing techniques used in NLP to reduce words to their base or root form, but they work in different ways.\n",
        "\n",
        "**Stemming** is a simple and fast process that removes prefixes or suffixes from words without considering the meaning of the word. It may sometimes produce incorrect or incomplete words. For example, the words *“playing,” “played,” and “plays”* may all be reduced to *“play.”* However, a word like *“studies”* might be reduced to *“studi,”* which is not a proper word.\n",
        "\n",
        "**Lemmatization**, on the other hand, is a more advanced and accurate method. It considers the meaning and context of the word and converts it into its correct base form (called a lemma). For example, *“running”* becomes *“run,”* and *“better”* becomes *“good.”* Lemmatization uses vocabulary and morphological analysis, so it usually produces meaningful words.\n",
        "\n",
        "In summary, stemming is faster but less accurate, while lemmatization is slower but more accurate and linguistically correct.\n"
      ],
      "metadata": {
        "id": "-mBmFnzviGeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "yQFWxlkyiODr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Given text\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regular expression pattern for extracting email addresses\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all email addresses\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "# Print extracted emails\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0extAGwiTQV",
        "outputId": "254388cd-ed03-4fc6-d9b5-a3db8a28fce9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical."
      ],
      "metadata": {
        "id": "-PPG9stRiaBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download punkt tokenizer (only needed once)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove punctuation tokens\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "print(words)\n",
        "\n",
        "# Print frequency distribution\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, frequency in freq_dist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJb4ZhsGicps",
        "outputId": "bc597039-4549-4598-8aab-ecf7a9437aa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'applications', 'of', 'nlp', 'include', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'as', 'technology', 'advances', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Frequency Distribution:\n",
            "natural: 1\n",
            "language: 2\n",
            "processing: 1\n",
            "nlp: 3\n",
            "is: 2\n",
            "a: 1\n",
            "fascinating: 1\n",
            "field: 1\n",
            "that: 1\n",
            "combines: 1\n",
            "linguistics: 1\n",
            "computer: 1\n",
            "science: 1\n",
            "and: 3\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "it: 1\n",
            "enables: 1\n",
            "machines: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "interpret: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "applications: 1\n",
            "of: 2\n",
            "include: 1\n",
            "chatbots: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "as: 1\n",
            "technology: 1\n",
            "advances: 1\n",
            "the: 1\n",
            "role: 1\n",
            "in: 1\n",
            "modern: 1\n",
            "solutions: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "critical: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given te"
      ],
      "metadata": {
        "id": "TwX91lZjimCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy first (if not installed)\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"John Doe works at Microsoft in New York. He recently visited India and met Sundar Pichai.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Proper Nouns Identified:\\n\")\n",
        "\n",
        "# Identify and label proper nouns\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        print(f\"{token.text} --> Proper Noun\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEE23nZeimye",
        "outputId": "cbeb3ce6-745b-4f74-c9f8-841618524cd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns Identified:\n",
            "\n",
            "John --> Proper Noun\n",
            "Doe --> Proper Noun\n",
            "Microsoft --> Proper Noun\n",
            "New --> Proper Noun\n",
            "York --> Proper Noun\n",
            "India --> Proper Noun\n",
            "Sundar --> Proper Noun\n",
            "Pichai --> Proper Noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim"
      ],
      "metadata": {
        "id": "O6krMLYvisNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gensim if not installed\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download punkt tokenizer (only once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Given dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Preprocessing: lowercase, tokenize, remove punctuation\n",
        "processed_data = []\n",
        "\n",
        "for sentence in dataset:\n",
        "    sentence = sentence.lower()\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # remove punctuation\n",
        "    processed_data.append(tokens)\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=100,   # size of word vectors\n",
        "    window=5,          # context window size\n",
        "    min_count=1,       # include all words\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# Example: Get vector for a word\n",
        "print(\"Vector for 'language':\\n\", model.wv['language'])\n",
        "\n",
        "# Example: Find similar words\n",
        "print(\"\\nWords similar to 'word':\")\n",
        "print(model.wv.most_similar('word'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwJKHLZPisxu",
        "outputId": "32fa2ae2-75b7-44d3-9339-4e6717cd9337"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Vector for 'language':\n",
            " [-9.5782308e-03  8.9439554e-03  4.1630305e-03  9.2365965e-03\n",
            "  6.6424119e-03  2.9268002e-03  9.8025510e-03 -4.4268500e-03\n",
            " -6.8041803e-03  4.2267689e-03  3.7317213e-03 -5.6662858e-03\n",
            "  9.7040217e-03 -3.5548718e-03  9.5477831e-03  8.3319913e-04\n",
            " -6.3350094e-03 -1.9779371e-03 -7.3799482e-03 -2.9804893e-03\n",
            "  1.0420545e-03  9.4868196e-03  9.3578557e-03 -6.5933121e-03\n",
            "  3.4730809e-03  2.2753577e-03 -2.4914318e-03 -9.2325537e-03\n",
            "  1.0250713e-03 -8.1645567e-03  6.3203699e-03 -5.8030877e-03\n",
            "  5.5381078e-03  9.8315831e-03 -1.5997297e-04  4.5317081e-03\n",
            " -1.8104383e-03  7.3636803e-03  3.9391587e-03 -9.0109985e-03\n",
            " -2.4005044e-03  3.6254451e-03 -1.0104214e-04 -1.1983733e-03\n",
            " -1.0542994e-03 -1.6753653e-03  6.0551270e-04  4.1602203e-03\n",
            " -4.2510536e-03 -3.8319139e-03 -5.1679679e-05  2.7000008e-04\n",
            " -1.6721402e-04 -4.7848546e-03  4.3132128e-03 -2.1718086e-03\n",
            "  2.1045052e-03  6.6549127e-04  5.9631816e-03 -6.8442305e-03\n",
            " -6.8144770e-03 -4.4765351e-03  9.4359918e-03 -1.5904348e-03\n",
            " -9.4318558e-03 -5.4289546e-04 -4.4510039e-03  5.9955167e-03\n",
            " -9.5842527e-03  2.8609247e-03 -9.2515424e-03  1.2468254e-03\n",
            "  5.9971227e-03  7.3962798e-03 -7.6207356e-03 -6.0502035e-03\n",
            " -6.8393731e-03 -7.9148887e-03 -9.5001869e-03 -2.1258055e-03\n",
            " -8.3689997e-04 -7.2583752e-03  6.7859967e-03  1.1175760e-03\n",
            "  5.8272253e-03  1.4769557e-03  7.9019315e-04 -7.3678466e-03\n",
            " -2.1758291e-03  4.3257722e-03 -5.0885999e-03  1.1301374e-03\n",
            "  2.8821314e-03 -1.5376916e-03  9.9321054e-03  8.3476044e-03\n",
            "  2.4147341e-03  7.1188016e-03  5.8868853e-03 -5.5812499e-03]\n",
            "\n",
            "Words similar to 'word':\n",
            "[('tokenization', 0.21890394389629364), ('modeling', 0.2162657529115677), ('embedding', 0.19546882808208466), ('processing', 0.16934241354465485), ('have', 0.15169860422611237), ('allows', 0.14183540642261505), ('with', 0.10882891714572906), ('technique', 0.09945283085107803), ('that', 0.09659130126237869), ('clean', 0.09340719133615494)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews\n",
        "\n",
        "Ans\n",
        "\n",
        "As a data scientist at a fintech startup, I would follow a structured NLP pipeline to analyze thousands of customer reviews and extract useful insights.\n",
        "\n",
        "First, I would **collect and organize the data**, ensuring all reviews are stored in a structured format like CSV or a database. Then, I would perform **data cleaning**, such as removing duplicates, handling missing values, converting text to lowercase, removing special characters, numbers, and unnecessary spaces.\n",
        "\n",
        "Next, I would apply **text preprocessing techniques** like tokenization, stop word removal, stemming or lemmatization, and text normalization to prepare the data for analysis. If needed, I would also perform spelling correction and remove irrelevant content like URLs or emojis.\n",
        "\n",
        "After preprocessing, I would perform **exploratory text analysis**, such as generating word frequency distributions, word clouds, and identifying common keywords or phrases. Then, I would apply **sentiment analysis** to classify reviews as positive, negative, or neutral to understand customer satisfaction levels.\n",
        "\n",
        "To gain deeper insights, I would use **topic modeling** (such as LDA) to identify common themes like payment issues, app performance, loan approvals, or customer support. I might also use **text classification models** to categorize feedback into predefined categories (e.g., complaints, feature requests, praise).\n",
        "\n",
        "Finally, I would visualize the results using dashboards and charts, highlight major pain points, track sentiment trends over time, and provide actionable insights to product and business teams. This approach would help the company improve services, reduce customer complaints, and enhance overall user experience.\n"
      ],
      "metadata": {
        "id": "9J8n3voii095"
      }
    }
  ]
}