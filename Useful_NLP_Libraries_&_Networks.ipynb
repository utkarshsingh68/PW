{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,\n",
        "and performance.\n",
        "\n",
        "Ans\n",
        "\n",
        "**NLTK (Natural Language Toolkit)** and **spaCy** are both popular NLP libraries in Python, but they differ in features, ease of use, and performance.\n",
        "\n",
        "In terms of **features**, NLTK is mainly used for educational and research purposes. It provides a wide range of tools for tokenization, stemming, lemmatization, parsing, and access to many linguistic datasets. It is very flexible and good for learning NLP concepts. spaCy, on the other hand, is designed for industrial and production use. It provides advanced features such as fast tokenization, part-of-speech tagging, named entity recognition (NER), dependency parsing, and pre-trained models out of the box.\n",
        "\n",
        "In terms of **ease of use**, NLTK is beginner-friendly and allows step-by-step implementation of NLP tasks, but it often requires more manual coding. spaCy is easier for building real-world applications because many tasks can be done with fewer lines of code using its pre-trained pipelines.\n",
        "\n",
        "In terms of **performance**, spaCy is generally faster and more efficient because it is written in Cython and optimized for speed. It is suitable for processing large datasets. NLTK is slower compared to spaCy and is better suited for small-scale projects or academic purposes.\n",
        "\n",
        "In summary, NLTK is ideal for learning and experimentation, while spaCy is better for high-performance, real-world NLP applications.\n"
      ],
      "metadata": {
        "id": "fQOBxriPkLJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
        "sentiment analysis and translation?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**TextBlob** is a simple and user-friendly Python library built on top of NLTK and Pattern. It is designed to make common NLP tasks easier and more accessible, especially for beginners.\n",
        "\n",
        "TextBlob simplifies tasks like **sentiment analysis** by providing built-in functions that directly return the polarity (positive/negative) and subjectivity of a sentence without requiring complex model training. For example, with just a few lines of code, you can get whether a review is positive, negative, or neutral.\n",
        "\n",
        "It also makes **translation** simple by providing a built-in translate() function that uses online translation services. Instead of building a machine translation model from scratch, users can translate text between languages easily.\n",
        "\n",
        "In addition to sentiment analysis and translation, TextBlob also supports tasks like tokenization, part-of-speech tagging, noun phrase extraction, and spelling correction with minimal code. Overall, TextBlob simplifies NLP by providing high-level APIs that reduce complexity and make implementation faster and easier.\n"
      ],
      "metadata": {
        "id": "iIDuLq36kP3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "The Stanford NLP Group plays a major role in both academic research and industry applications of Natural Language Processing. It is one of the leading research groups in NLP and has contributed many important tools, models, and research papers that are widely used worldwide.\n",
        "\n",
        "In academic projects, Stanford NLP provides open-source tools like Stanford CoreNLP and Stanza, which are commonly used for tasks such as tokenization, part-of-speech tagging, named entity recognition (NER), dependency parsing, and sentiment analysis. Many researchers and students use these tools for experiments, thesis work, and advanced NLP research because they are reliable and scientifically validated.\n",
        "\n",
        "In industry projects, Stanford NLP tools are used to build real-world applications such as chatbots, information extraction systems, question-answering systems, and text analytics platforms. The research from Stanford has also influenced the development of modern deep learning models and transformer-based architectures used in companies like Google, Microsoft, and OpenAI.\n",
        "\n",
        "Overall, Stanford NLP acts as a bridge between research and practical implementation, contributing foundational theories as well as production-ready NLP tools for both academia and industry."
      ],
      "metadata": {
        "id": "T05BU9ewkaAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Describe the architecture and functioning of a Recurrent Natural Network\n",
        "(RNN).\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network designed to process sequential data such as text, speech, or time series data. Unlike traditional neural networks, RNNs have a special feature called a hidden state, which allows them to remember information from previous time steps.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "An RNN consists of:\n",
        "\n",
        "Input layer (xₜ) – Takes input at each time step (for example, each word in a sentence).\n",
        "\n",
        "Hidden layer (hₜ) – Stores information from the current input and the previous hidden state.\n",
        "\n",
        "Output layer (yₜ) – Produces the output at each time step.\n",
        "\n",
        "The key idea is that the hidden state is passed from one time step to the next. This creates a loop (or recurrence), which allows the network to maintain memory of previous inputs.\n",
        "\n",
        "Functioning:\n",
        "\n",
        "At each time step:\n",
        "\n",
        "The network takes the current input (xₜ).\n",
        "\n",
        "It combines it with the previous hidden state (hₜ₋₁).\n",
        "\n",
        "It computes the new hidden state (hₜ).\n",
        "\n",
        "Then it generates the output (yₜ)."
      ],
      "metadata": {
        "id": "GQoQT_FTkmTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is the key difference between LSTM and GRU networks in NLP\n",
        "applications?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "The key difference between LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks lies in their internal structure and number of gates used to control information flow.\n",
        "\n",
        "LSTM has a more complex architecture with three gates:\n",
        "\n",
        "Forget Gate – Decides what information to remove from memory.\n",
        "\n",
        "Input Gate – Decides what new information to store.\n",
        "\n",
        "Output Gate – Decides what information to pass to the next step.\n",
        "\n",
        "It also maintains a separate cell state, which helps store long-term information more effectively. Because of this structure, LSTMs are powerful for capturing long-term dependencies but are computationally heavier.\n",
        "\n",
        "GRU, on the other hand, is a simplified version of LSTM. It has only two gates:\n",
        "\n",
        "Update Gate – Combines the functions of forget and input gates.\n",
        "\n",
        "Reset Gate – Controls how much past information to forget.\n",
        "\n",
        "GRU does not have a separate cell state; it merges memory and hidden state together. As a result, GRUs are faster to train, require fewer parameters, and perform similarly to LSTMs in many NLP tasks."
      ],
      "metadata": {
        "id": "ZVl5IG1nkwAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
        "the following paragraph of text:\n",
        "“I had a great experience using the new mobile banking app. The interface is intuitive,\n",
        "and customer support was quick to resolve my issue. However, the app did crash once\n",
        "during a transaction, which was frustrating\"\n",
        "Your program should print out the polarity and subjectivity scores.\n"
      ],
      "metadata": {
        "id": "lwp0sVhxk2PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TextBlob if not installed\n",
        "# pip install textblob\n",
        "# python -m textblob.download_corpora\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating.\"\"\"\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform Sentiment Analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Print results\n",
        "print(\"Polarity:\", sentiment.polarity)\n",
        "print(\"Subjectivity:\", sentiment.subjectivity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HDshGBBk9GN",
        "outputId": "9dea8b74-9e9a-4701-f0be-8eb2e4db0db0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”"
      ],
      "metadata": {
        "id": "RFTv1rgklGCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK if not installed\n",
        "# pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "\n",
        "# Download tokenizer (only first time)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to address the LookupError\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove punctuation\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "print(words)\n",
        "\n",
        "# Print frequency distribution\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, count in freq_dist.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4m-5_M6lGpW",
        "outputId": "d14e74b8-f51c-4056-ba4f-e183d2f06b6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'applications', 'of', 'nlp', 'include', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'as', 'technology', 'advances', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Frequency Distribution:\n",
            "natural: 1\n",
            "language: 2\n",
            "processing: 1\n",
            "nlp: 3\n",
            "is: 2\n",
            "a: 1\n",
            "fascinating: 1\n",
            "field: 1\n",
            "that: 1\n",
            "combines: 1\n",
            "linguistics: 1\n",
            "computer: 1\n",
            "science: 1\n",
            "and: 3\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "it: 1\n",
            "enables: 1\n",
            "machines: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "interpret: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "applications: 1\n",
            "of: 2\n",
            "include: 1\n",
            "chatbots: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "as: 1\n",
            "technology: 1\n",
            "advances: 1\n",
            "the: 1\n",
            "role: 1\n",
            "in: 1\n",
            "modern: 1\n",
            "solutions: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "critical: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
        "the following dummy dataset. Your model should classify sentences as either positive\n",
        "(1) or negative (0).\n",
        "# Dataset\n",
        "texts = [\n",
        "“I love this project”, #Positive\n",
        "“This is an amazing experience”, #Positive\n",
        "“I hate waiting in line”, #Negative\n",
        "“This is the worst service”, #Negative\n",
        "“Absolutely fantastic!” #Positive\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
        "this data. You may use Keras with TensorFlow backend.\n"
      ],
      "metadata": {
        "id": "fvPn2BqClLsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow if not installed\n",
        "# pip install tensorflow\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# ----------------------\n",
        "# Dataset\n",
        "# ----------------------\n",
        "texts = [\n",
        "    \"I love this project\",                  # Positive\n",
        "    \"This is an amazing experience\",        # Positive\n",
        "    \"I hate waiting in line\",               # Negative\n",
        "    \"This is the worst service\",            # Negative\n",
        "    \"Absolutely fantastic!\"                 # Positive\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "# ----------------------\n",
        "# Text Preprocessing\n",
        "# ----------------------\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# ----------------------\n",
        "# Build LSTM Model\n",
        "# ----------------------\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# Train Model\n",
        "# ----------------------\n",
        "model.fit(padded_sequences, labels, epochs=20, verbose=1)\n",
        "\n",
        "# ----------------------\n",
        "# Test Prediction\n",
        "# ----------------------\n",
        "test_text = [\"I really love this service\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model.predict(test_pad)\n",
        "print(\"Prediction Probability:\", prediction[0][0])\n",
        "print(\"Predicted Sentiment:\", 1 if prediction[0][0] > 0.5 else 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56xt7nCjlRHe",
        "outputId": "a69402c7-5de4-40f1-8d1e-cbefbd7c5c1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.6909\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.6890\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8000 - loss: 0.6871\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8000 - loss: 0.6851\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6000 - loss: 0.6831\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6000 - loss: 0.6811\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.6000 - loss: 0.6789\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6000 - loss: 0.6767\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.6000 - loss: 0.6745\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6000 - loss: 0.6721\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.6000 - loss: 0.6697\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6672\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6000 - loss: 0.6646\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6000 - loss: 0.6618\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6590\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6561\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6530\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6000 - loss: 0.6498\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6465\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6431\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "Prediction Probability: 0.52232504\n",
            "Predicted Sentiment: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
        "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "“Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.”\n",
        "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
        "lemmas, and any named entities found"
      ],
      "metadata": {
        "id": "ocPTRuMplWm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy if not installed\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# -------------------------\n",
        "# Tokenization & Lemmatization\n",
        "# -------------------------\n",
        "print(\"Tokens and Lemmas:\\n\")\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"Token: {token.text:<20} Lemma: {token.lemma_}\")\n",
        "\n",
        "# -------------------------\n",
        "# Named Entity Recognition\n",
        "# -------------------------\n",
        "print(\"\\nNamed Entities:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text:<45} Label: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mispa9dulZ24",
        "outputId": "a0a2ec33-f74d-4378-93ff-5c89481523e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and Lemmas:\n",
            "\n",
            "Token: Homi                 Lemma: Homi\n",
            "Token: Jehangir             Lemma: Jehangir\n",
            "Token: Bhaba                Lemma: Bhaba\n",
            "Token: was                  Lemma: be\n",
            "Token: an                   Lemma: an\n",
            "Token: Indian               Lemma: indian\n",
            "Token: nuclear              Lemma: nuclear\n",
            "Token: physicist            Lemma: physicist\n",
            "Token: who                  Lemma: who\n",
            "Token: played               Lemma: play\n",
            "Token: a                    Lemma: a\n",
            "Token: key                  Lemma: key\n",
            "Token: role                 Lemma: role\n",
            "Token: in                   Lemma: in\n",
            "Token: the                  Lemma: the\n",
            "Token: development          Lemma: development\n",
            "Token: of                   Lemma: of\n",
            "Token: India                Lemma: India\n",
            "Token: ’s                   Lemma: ’s\n",
            "Token: atomic               Lemma: atomic\n",
            "Token: energy               Lemma: energy\n",
            "Token: program              Lemma: program\n",
            "Token: He                   Lemma: he\n",
            "Token: was                  Lemma: be\n",
            "Token: the                  Lemma: the\n",
            "Token: founding             Lemma: found\n",
            "Token: director             Lemma: director\n",
            "Token: of                   Lemma: of\n",
            "Token: the                  Lemma: the\n",
            "Token: Tata                 Lemma: Tata\n",
            "Token: Institute            Lemma: Institute\n",
            "Token: of                   Lemma: of\n",
            "Token: Fundamental          Lemma: Fundamental\n",
            "Token: Research             Lemma: Research\n",
            "Token: TIFR                 Lemma: TIFR\n",
            "Token: and                  Lemma: and\n",
            "Token: was                  Lemma: be\n",
            "Token: instrumental         Lemma: instrumental\n",
            "Token: in                   Lemma: in\n",
            "Token: establishing         Lemma: establish\n",
            "Token: the                  Lemma: the\n",
            "Token: Atomic               Lemma: Atomic\n",
            "Token: Energy               Lemma: Energy\n",
            "Token: Commission           Lemma: Commission\n",
            "Token: of                   Lemma: of\n",
            "Token: India                Lemma: India\n",
            "\n",
            "Named Entities:\n",
            "\n",
            "Entity: Homi Jehangir Bhaba                           Label: FAC\n",
            "Entity: Indian                                        Label: NORP\n",
            "Entity: India                                         Label: GPE\n",
            "Entity: the Tata\n",
            "Institute of Fundamental Research    Label: ORG\n",
            "Entity: Atomic Energy Commission of India             Label: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
        "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
        "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
        "preprocessing pipeline, and any ethical considerations.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "To build a chatbot for a mental health platform, I would design a system that combines spaCy (or Stanford NLP tools) for language understanding and LSTM/GRU networks for response prediction.\n",
        "\n",
        "1️  Overall Architecture\n",
        "\n",
        "The chatbot system would have the following components:\n",
        "\n",
        "User Input → Preprocessing → NLP Understanding → LSTM/GRU Model → Response Generator → Output\n",
        "\n",
        "Components:\n",
        "\n",
        "spaCy / Stanford NLP → For linguistic analysis (tokenization, POS tagging, entity recognition)\n",
        "\n",
        "LSTM or GRU Model → For intent classification and context understanding\n",
        "\n",
        "Response Module → Rule-based or generative response system\n",
        "\n",
        "Database / Knowledge Base → Mental health resources and safe responses\n",
        "\n",
        "2️  Data Preprocessing Pipeline\n",
        "\n",
        "Using spaCy or Stanford NLP, I would:\n",
        "\n",
        "Tokenization – Split text into words.\n",
        "\n",
        "Lemmatization – Convert words to base form.\n",
        "\n",
        "Stopword Removal – Remove unnecessary words (if needed).\n",
        "\n",
        "Named Entity Recognition (NER) – Detect names, locations, or sensitive mentions.\n",
        "\n",
        "Dependency Parsing – Understand sentence structure.\n",
        "\n",
        "Example:\n",
        "Input → \"I feel anxious and can’t sleep lately.\"\n",
        "Extracted intent → emotional distress\n",
        "Keywords → anxious, sleep\n",
        "\n",
        "Then:\n",
        "\n",
        "Convert tokens into sequences\n",
        "\n",
        "Apply padding\n",
        "\n",
        "Convert into word embeddings (Word2Vec, GloVe, or embedding layer)\n",
        "\n",
        "3️  Role of LSTM or GRU\n",
        "\n",
        "Mental health conversations require understanding context over time. That’s why LSTM or GRU is useful.\n",
        "\n",
        "Why LSTM/GRU?\n",
        "\n",
        "They remember previous messages in a conversation.\n",
        "\n",
        "They capture emotional patterns across sentences.\n",
        "\n",
        "They handle sequential dependencies better than simple neural networks.\n",
        "\n",
        "Model Tasks:\n",
        "\n",
        "Intent classification (e.g., anxiety, depression, crisis, general stress)\n",
        "\n",
        "Sentiment detection\n",
        "\n",
        "Emotion detection\n",
        "\n",
        "Context-aware response selection\n",
        "\n",
        "GRU may be preferred if:\n",
        "\n",
        "Faster training is required\n",
        "\n",
        "Limited computational resources\n",
        "\n",
        "LSTM may be preferred if:\n",
        "\n",
        "Long-term conversational context is critical\n",
        "\n",
        "4️  Response Generation Strategy\n",
        "\n",
        "Two approaches:\n",
        "\n",
        " Retrieval-Based\n",
        "\n",
        "Match detected intent to predefined safe therapeutic responses.\n",
        "\n",
        "Use mental health knowledge base.\n",
        "\n",
        "More controlled and safer.\n",
        "\n",
        " Generative Model (LSTM/GRU Decoder)\n",
        "\n",
        "Generate responses dynamically.\n",
        "\n",
        "Must be heavily monitored to avoid harmful outputs.\n",
        "\n",
        "For mental health, I would prefer retrieval-based + safety filters.\n",
        "\n",
        "5️ Safety & Ethical Considerations ⚠️\n",
        "\n",
        "This is extremely important in mental health applications:\n",
        " 1. Crisis Detection\n",
        "\n",
        "Detect phrases like “I want to harm myself”\n",
        "\n",
        "Immediately escalate to crisis helpline information.\n",
        "\n",
        "2. Bias Mitigation\n",
        "\n",
        "Ensure model is trained on diverse datasets.\n",
        "\n",
        "Avoid gender, cultural, or racial bias.\n",
        "\n",
        " 3. Privacy & Data Protection\n",
        "\n",
        "Encrypt user conversations.\n",
        "\n",
        "Follow GDPR / data protection standards.\n",
        "\n",
        "No storing sensitive data without consent.\n",
        "\n",
        " 4. Transparency\n",
        "\n",
        "Clearly inform users that chatbot is not a licensed therapist.\n",
        "\n",
        "Provide disclaimers.\n",
        "\n",
        "5. Human-in-the-loop\n",
        "\n",
        "Escalate severe cases to human counselors.\n",
        "\n",
        "6️ Final Architecture Summary\n",
        "\n",
        "spaCy / Stanford NLP → Linguistic understanding\n",
        "\n",
        "Embedding Layer → Word representation\n",
        "\n",
        "LSTM / GRU Network → Context modeling\n",
        "\n",
        "Dense + Softmax Layer → Intent classification\n",
        "\n",
        "Response Engine → Safe, empathetic replies\n",
        "\n",
        "Crisis Escalation Module → Safety mechanism"
      ],
      "metadata": {
        "id": "GgS4fQzilbYH"
      }
    }
  ]
}