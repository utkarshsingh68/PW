{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Ensemble Learning** is a machine learning technique where multiple models are combined to make better predictions than a single model. Instead of relying on one model, ensemble methods use a group of models and combine their outputs to improve accuracy and reliability.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of weak or simple models can work together to create a strong model. Each model may make different errors, but when their predictions are combined (through methods like voting or averaging), the overall error is reduced.\n",
        "\n",
        "There are different types of ensemble methods, such as:\n",
        "\n",
        "* **Bagging** (e.g., Random Forest), where models are trained independently and their predictions are averaged.\n",
        "* **Boosting** (e.g., AdaBoost, Gradient Boosting), where models are trained sequentially and each new model focuses on correcting previous errors.\n",
        "* **Stacking**, where multiple models are combined using another model.\n",
        "\n",
        "In simple terms, ensemble learning improves performance by combining multiple models to get more accurate and stable predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "H5PSpzHq1IDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Bagging** and **Boosting** are both ensemble learning techniques, but they differ in how models are trained and combined.\n",
        "\n",
        "**Bagging (Bootstrap Aggregating)** trains multiple models independently using different random samples of the dataset. Each model learns separately, and their predictions are combined using averaging (for regression) or majority voting (for classification). Bagging mainly reduces variance and helps prevent overfitting. A common example is Random Forest.\n",
        "\n",
        "**Boosting**, on the other hand, trains models sequentially. Each new model focuses more on the errors made by the previous model. It gives more importance (weight) to misclassified data points and tries to correct them. Boosting mainly reduces bias and improves overall accuracy. Examples include AdaBoost and Gradient Boosting.\n",
        "\n",
        "In simple terms, Bagging builds models independently and combines them, while Boosting builds models step by step, correcting previous mistakes.\n"
      ],
      "metadata": {
        "id": "LtiW30Kr1LOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Bootstrap sampling** is a technique where we create multiple new datasets by randomly selecting samples from the original dataset **with replacement**. “With replacement” means the same data point can be selected more than once in a sample, and some data points may not be selected at all.\n",
        "\n",
        "In **Bagging** methods like **Random Forest**, bootstrap sampling plays a very important role. Each decision tree in the Random Forest is trained on a different bootstrap sample of the original dataset. Since each tree sees slightly different data, they learn different patterns and make different errors.\n",
        "\n",
        "After training, the predictions from all trees are combined using majority voting (for classification) or averaging (for regression). This reduces variance and improves model stability.\n",
        "\n",
        "In simple terms, bootstrap sampling helps create diversity among models in Random Forest, which makes the final combined model more accurate and less prone to overfitting.\n"
      ],
      "metadata": {
        "id": "OGoq7qPe1T3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "**Out-of-Bag (OOB) samples** are the data points that are not selected in a particular bootstrap sample during training in Bagging methods like Random Forest. Since bootstrap sampling is done with replacement, some data points are left out when creating each training subset. These unused data points are called Out-of-Bag samples.\n",
        "\n",
        "In ensemble models like Random Forest, each tree is trained on its own bootstrap sample, and the OOB samples for that tree are used as a small validation set. After the tree is trained, it makes predictions on its OOB samples. This process is repeated for all trees, and the predictions are combined to calculate the **OOB score**.\n",
        "\n",
        "The **OOB score** works like a built-in validation accuracy. It provides an estimate of the model’s performance without needing a separate validation dataset. This helps evaluate how well the ensemble model is likely to perform on unseen data.\n",
        "\n",
        "In simple terms, OOB samples are leftover data used to test the model during training, and the OOB score gives an internal measure of model accuracy.\n"
      ],
      "metadata": {
        "id": "HzaSJX2n1aN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "Feature importance analysis tells us which features are most useful in making predictions.\n",
        "\n",
        "In a **single Decision Tree**, feature importance is calculated based on how much each feature reduces impurity (such as Gini or Entropy) when it is used for splitting. If a feature is used near the top of the tree and creates strong splits, it gets higher importance. However, since it depends on just one tree, the importance values can be unstable. Small changes in data can result in a different tree and different feature importance.\n",
        "\n",
        "In a **Random Forest**, feature importance is calculated by averaging the importance across many decision trees. Each tree is trained on different bootstrap samples and random feature subsets, so the importance scores are more stable and reliable. Random Forest reduces the risk of overfitting and provides a more robust estimate of which features truly matter.\n",
        "\n",
        "In simple terms, a single Decision Tree gives feature importance based on one model, which may not always be stable, while Random Forest gives more reliable and consistent feature importance by combining results from many trees.\n"
      ],
      "metadata": {
        "id": "PJI_j0xN1daI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "_9wecIsj1pxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Create DataFrame for feature importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by=\"Importance\", ascending=False\n",
        ")\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuushZPm1qeM",
        "outputId": "d516e1d9-5ffa-4b94-af0c-577c23eaaff9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "7    mean concave points    0.141934\n",
            "27  worst concave points    0.127136\n",
            "23            worst area    0.118217\n",
            "6         mean concavity    0.080557\n",
            "20          worst radius    0.077975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "grUmEloc1uzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Single Decision Tree\n",
        "# -----------------------------\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# -----------------------------\n",
        "# Bagging Classifier with Decision Trees\n",
        "# -----------------------------\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Single Decision Tree:\", accuracy_dt)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOhzj06Z1vcP",
        "outputId": "398d9b2f-7fb7-4997-fe23-5831d8960790"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "WaF3pchH15EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate final accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg254qUt160R",
        "outputId": "f9d7c95a-8519-4703-f061-908e3e217ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "OE9ylOvP2Mmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Bagging Regressor\n",
        "# -----------------------------\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# -----------------------------\n",
        "# Random Forest Regressor\n",
        "# -----------------------------\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwIVDQOm2NHp",
        "outputId": "13c1bd4b-4236-4a22-abc7-d6e5015e7ba6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "Ans\n",
        "\n",
        "If I am working at a financial institution to predict loan default, I would follow a structured approach using ensemble techniques to improve prediction accuracy and reduce risk.\n",
        "\n",
        "Step 1: Choose Between Bagging and Boosting\n",
        "\n",
        "First, I would understand the dataset size, complexity, and risk level.\n",
        "\n",
        "* If the dataset is large and noisy, and I want to reduce variance and overfitting, I would use Bagging (for example, Random Forest).\n",
        "* If the problem is complex and I want to reduce bias and improve accuracy by focusing on difficult cases, I would use Boosting (for example, Gradient Boosting or XGBoost).\n",
        "\n",
        "Since loan default prediction is usually a high-stakes problem where accuracy is very important, Boosting is often preferred because it improves performance by correcting errors step by step. However, I would test both methods before final selection.\n",
        "\n",
        "Step 2: Handle Overfitting\n",
        "\n",
        "To control overfitting, I would:\n",
        "\n",
        "* Use cross-validation.\n",
        "* Tune hyperparameters such as max_depth, learning rate (for boosting), and number of estimators.\n",
        "* Use regularization parameters.\n",
        "* Monitor validation performance instead of only training accuracy.\n",
        "* Apply early stopping in boosting models.\n",
        "\n",
        "Step 3: Select Base Models\n",
        "\n",
        "For Bagging:\n",
        "\n",
        "* Decision Trees are commonly used as base learners.\n",
        "\n",
        "For Boosting:\n",
        "\n",
        "* Shallow Decision Trees (weak learners) are typically used.\n",
        "* Algorithms like XGBoost, LightGBM, or Gradient Boosting can be applied.\n",
        "\n",
        "Decision Trees are suitable because they handle mixed data types (numerical and categorical) and capture non-linear relationships in financial data.\n",
        "\n",
        "Step 4: Evaluate Performance Using Cross-Validation\n",
        "\n",
        "I would use k-fold cross-validation to evaluate model stability. Important evaluation metrics for loan default prediction include:\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* ROC-AUC score\n",
        "\n",
        "In financial risk prediction, Recall (identifying defaulters correctly) is especially important because missing a defaulter can cause financial loss.\n",
        "\n",
        "Step 5: Business Justification of Ensemble Learning\n",
        "\n",
        "Ensemble learning improves decision-making in loan default prediction by:\n",
        "\n",
        "* Increasing prediction accuracy.\n",
        "* Reducing risk of approving high-risk customers.\n",
        "* Providing more stable and reliable results.\n",
        "* Handling complex customer behavior patterns.\n",
        "* Supporting data-driven credit approval decisions.\n",
        "\n",
        "In real-world finance, better predictions lead to reduced bad loans, improved profitability, and better risk management. Ensemble models provide stronger and more reliable performance compared to a single model, which makes them valuable for critical financial decisions.\n"
      ],
      "metadata": {
        "id": "Mai9QZ_q2TPb"
      }
    }
  ]
}